# -*- coding: utf-8 -*-
"""BertTopic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KCVlB5nacrol4fXyE7mTHyUMaHirlQvO
"""

# Dependencias
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 -q
!pip install -U bertopic[visualization] sentence-transformers umap-learn hdbscan mlflow -q
!pip install -q fastapi uvicorn nest-asyncio pyngrok

import os
import time
import mlflow
import pandas as pd
import numpy as np
from umap import UMAP
from hdbscan import HDBSCAN
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer

from google.colab import drive

import re
import nltk
from nltk.corpus import stopwords

!pip freeze

# LINK DRIVE
drive.mount('/content/drive')

# PATHs CONFIG
DATA_PATH = "/content/drive/MyDrive/train1.tsv"

PROJECT_DIR = "/content/drive/MyDrive/bertopic_experimentos"
os.makedirs(PROJECT_DIR, exist_ok=True)

MODEL_DIR = os.path.join(PROJECT_DIR, "bertopic_model")
EMBEDDINGS_PATH = os.path.join(PROJECT_DIR, "precomputed_embeddings.npy")
VIS_PATH = os.path.join(PROJECT_DIR, "bertopic_viz.html")
BARCHART_PATH = os.path.join(PROJECT_DIR, "bertopic_barchart.html")
HEATMAP_PATH = os.path.join(PROJECT_DIR, "bertopic_heatmap.html")

#READ DATA SET
df_train = pd.read_csv(DATA_PATH, sep='\t')
df_train.dropna(subset=["title", "content"], inplace=True)
df_train["text"] = df_train["title"] + " " + df_train["content"]
#df_train.drop(columns=["title", "content"], inplace=True)
print(df_train.head())

nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('omw-1.4', quiet=True)

stop_words_en = set(stopwords.words('english'))

def clean_text_for_ner(text):
    """
    Limpia el texto eliminando URLs y caracteres especiales innecesarios,
    manteniendo las mayÃºsculas para el NER.
    """
    text = re.sub(r'http\S+', '', text)  # Elimina URLs
    text = re.sub(r'[^a-zA-Z\s.,!?"\'()]', '', text) # Mantiene letras, espacios y signos de puntuaciÃ³n comunes
    text = re.sub(r'@\s*@\s*@\s*@\s*', '', text) # Elimina secuencias de @ repetidas con espacios opcionales
    return text

def remove_stopwords(text):
    """Elimina las stopwords del texto."""
    words = text.split()
    filtered_words = [word for word in words if word.lower() not in stop_words_en] # Convertir a minÃºsculas para la comparaciÃ³n
    return " ".join(filtered_words)

def preprocess_for_ner_inplace(df, text_column):
    """
    Aplica la limpieza y eliminaciÃ³n de stopwords directamente sobre la columna de texto del DataFrame.
    No se aplica lematizaciÃ³n para el NER.
    """
    df[text_column] = df[text_column].apply(remove_stopwords)
    df[text_column] = df[text_column].apply(clean_text_for_ner)
    return df

df_train = preprocess_for_ner_inplace(df_train, 'text')

print("\nDataFrame despuÃ©s del preprocesamiento inplace para NER:")
print(df_train[['text']].head())

#MLFLOW
# Paso 1: MLflow local conectado a colab internet
mlflow.set_tracking_uri("https://58af-46-6-49-108.ngrok-free.app")

def mlflow_log_params():
  mlflow.log_param("embedding_model", "all-mpnet-base-v2")

  #Dataset
  mlflow.log_param("dataset_size_MB", round(df_train.memory_usage(deep=True).sum() / 1e6, 2))
  mlflow.log_param("n_docs", len(df_train))

  #UMAP
  mlflow.log_param("n_neighbors_umap", umap_n_neighbors)
  mlflow.log_param("n_components_umap", umap_n_components)
  mlflow.log_param("min_dist_umap", umap_min_dist)
  mlflow.log_param("metric_umap", umap_metric)

  #HDBSCAN
  mlflow.log_param("hdbscan_min_cluster_size", hdbscan_min_cluster_size)
  mlflow.log_param("hdbscan_min_samples", hdbscan_min_samples)
  mlflow.log_param("hdbscan_metric", hdbscan_metric)
  mlflow.log_param("hdbscan_cluster_selection_method", hdbscan_cluster_selection_method)

  #RESULTS
  mlflow.log_param("n_topics", num_topics)

  #TIMES
  mlflow.log_metric("train_time_sec", duration)
  mlflow.log_metric("silhouette_score", silhouette)
  mlflow.log_metric("calinski_harabasz", ch_score)
  mlflow.log_metric("diversity_score", diversity)

  mlflow.log_artifact(VIS_PATH)
  mlflow.log_artifact(MODEL_DIR)

#Modelo de embedding
model_name = "all-mpnet-base-v2"
embedding_model = SentenceTransformer(model_name)
precomputed_embeddings = embedding_model.encode(df_train["text"].tolist(), show_progress_bar=True)

np.save(EMBEDDINGS_PATH, precomputed_embeddings)

# ConfiguraciÃ³n UMAP
umap_min_dist = 0.1
umap_n_neighbors = 75
umap_metric = 'cosine'
umap_n_components = 20

umap_model = UMAP(n_neighbors=umap_n_neighbors,
                  n_components=umap_n_components,
                  min_dist=umap_min_dist,
                  metric=umap_metric,
                  random_state=42)

#Configuracion HDBSCAN
hdbscan_min_samples = 10
hdbscan_metric = 'euclidean'
hdbscan_min_cluster_size = 40
hdbscan_cluster_selection_method = 'eom'

hdbscan_model = HDBSCAN(min_cluster_size=hdbscan_min_cluster_size,
                        min_samples=hdbscan_min_samples,
                        metric=hdbscan_metric,
                        cluster_selection_method=hdbscan_cluster_selection_method,
                        prediction_data=True)

#silhouette score
from sklearn.metrics import silhouette_score, calinski_harabasz_score
def metrics():
  valid_idxs = [i for i, t in enumerate(topics) if t != -1]
  X_valid = [precomputed_embeddings[i] for i in valid_idxs]
  labels_valid = [topics[i] for i in valid_idxs]

  # SILHOUETTE SCORE
  if len(set(labels_valid)) > 1:
    silhouette = silhouette_score(X_valid, labels_valid)
  # CALINSKI-HARABASZ
  if len(set(labels_valid)) > 1:
    ch_score = calinski_harabasz_score(X_valid, labels_valid)

  # DIVERSIDAD LÃ‰XICA
  topics_words = [
    [word for word, _ in topic]
    for topic in topic_model.get_topics().values() if topic
  ]
  flat_words = [word for topic in topics_words for word in topic]
  unique_words = set(flat_words)
  diversity = len(unique_words) / len(flat_words) if flat_words else 0

  return silhouette, ch_score, diversity

mlflow.set_experiment("noticias-temas-bertopic")
with mlflow.start_run():
  start = time.time()
  # Entrenamiento BERTopic
  topic_model = BERTopic(
      embedding_model=embedding_model,
      umap_model=umap_model,
      hdbscan_model=hdbscan_model,
      nr_topics='auto',
      verbose=True
  )
  topics, probs = topic_model.fit_transform(df_train["text"].tolist(), embeddings=precomputed_embeddings)

  duration = round(time.time() - start, 2)
  num_topics = len(set(topics)) - (1 if -1 in topics else 0)
  silhouette, ch_score, diversity = metrics()
  #MLFLOW
  mlflow_log_params()

umap_model_viz_3d = UMAP(
    n_neighbors=15,
    n_components=3,
    min_dist=0.1,
    metric='cosine',
    random_state=42,
    verbose=True
)

embeddings_3d = umap_model_viz_3d.fit_transform(precomputed_embeddings)

print(f"Forma de embeddings_3d: {embeddings_3d.shape}")

import plotly.express as px
df_viz = pd.DataFrame(embeddings_3d, columns=["x", "y", "z"])
df_viz["topic"] = topics

fig = px.scatter_3d(
    df_viz,
    x="x", y="y", z="z",
    color="topic",
    title="VisualizaciÃ³n 3D de los documentos por tÃ³pico",
    width=1200,
    height=800
)

fig.show()

print(f"âœ… Entrenamiento completado en {duration}s - {num_topics} temas detectados")

topic_model.get_topic_info()[['Topic', 'Count', 'Representation']]

topics_reducidos_prob = topic_model.reduce_outliers(df_train["text"].tolist(),topics, strategy="embeddings", embeddings=precomputed_embeddings,threshold=0.4)

print(f"Documentos en Tema -1 DESPUÃ‰S de 'probabilities': {topics_reducidos_prob.count(-1)}")

import copy
# Hacer una copia profunda del modelo original
topic_model_copy = copy.deepcopy(topic_model)

topic_model.update_topics(df_train["text"].tolist(), topics=topics_reducidos_prob)
topic_model.get_topic_info()[['Topic', 'Count', 'Representation']]

#  VisualizaciÃ³n
vis_html = topic_model.visualize_topics()
vis_html.write_html(VIS_PATH)

topic_model.save(MODEL_DIR)

try:
    barchart_fig = topic_model.visualize_barchart(top_n_topics=len(set(topics)))
    barchart_fig.write_html(BARCHART_PATH)
    print(f"GrÃ¡fico de barras guardado en: {BARCHART_PATH}")
    barchart_fig.show()
except Exception as e:
    print(f"Error generando barchart: {e}")

try:
  heatmap_fig = topic_model.visualize_heatmap()
  heatmap_fig.write_html(HEATMAP_PATH)
  print(f"Heatmap guardado en: {HEATMAP_PATH}")
  heatmap_fig.show()
except Exception as e:
  print(f"Error generando heatmap: {e}")

"""ENDPOINT"""

model = BERTopic.load(MODEL_DIR)

from fastapi import FastAPI
from pydantic import BaseModel
from typing import List
import nest_asyncio
import uvicorn
from pyngrok import ngrok, conf

nest_asyncio.apply()
conf.get_default().auth_token = "2xlOx9TdwDwzr6ldxJZB7F5crh3_3ZEJ2xurVzmVnJqq25MYF"
app = FastAPI()

class Document(BaseModel):
    id: int
    text: str

class InferenceRequest(BaseModel):
    documents: List[Document]

@app.post("/predict")
def predict(request: InferenceRequest):
    texts = [doc.text for doc in request.documents]
    ids = [doc.id for doc in request.documents]

    topics, _ = model.transform(texts)

    return [{"id": int(id_), "topic": int(topic)} for id_, topic in zip(ids, topics)]

# Abre tÃºnel ngrok al puerto 5000
public_url = ngrok.connect(5000)
print(f"ðŸ”— Tu endpoint pÃºblico es: {public_url}")

# Lanza el servidor FastAPI
uvicorn.run(app, host="0.0.0.0", port=5000)

