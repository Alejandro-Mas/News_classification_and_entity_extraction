# -*- coding: utf-8 -*-
"""NER_Extraction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S7Srrdn3-udXsvRw2e8bTcgi0Np41qI3
"""

import os
import re
import spacy
import nltk
from nltk.corpus import stopwords
import pandas as pd
from google.colab import drive

# LINK DRIVE
drive.mount('/content/drive')

# PATHs CONFIG
DATA_PATH = "/content/drive/MyDrive/bertopic_experimentos/final_topic_dataset.tsv"

PROJECT_DIR = "/content/drive/MyDrive/ner_extraction"

TSV_DIR = os.path.join(PROJECT_DIR, "ner_results.tsv")
os.makedirs(PROJECT_DIR, exist_ok=True)

MODEL_DIR = os.path.join(PROJECT_DIR, "ner_model")

MODEL_NAME_EN = "en_core_web_md"
spacy.cli.download(MODEL_NAME_EN)
nlp_en = spacy.load(MODEL_NAME_EN)

#READ DATA SET
df_train = pd.read_csv(DATA_PATH, sep='\t')
df_train.dropna(subset=["title", "content"], inplace=True)
df_train["text"] = df_train["title"] + " " + df_train["content"]
#df_train.drop(columns=["title", "content"], inplace=True)
print(df_train.head())

def clean_text_for_ner(text):
    """
    Limpia el texto eliminando URLs y caracteres especiales innecesarios,
    manteniendo las mayúsculas para el NER.
    """
    text = re.sub(r'http\S+', '', text)  # Elimina URLs
    text = re.sub(r'[^a-zA-Z\s.,!?"\'()]', '', text) # Mantiene letras, espacios y signos de puntuación comunes
    text = re.sub(r'@\s*@\s*@\s*@\s*', '', text) # Elimina secuencias de @ repetidas con espacios opcionales
    return text

def remove_stopwords(text):
    """Elimina las stopwords del texto."""
    words = text.split()
    filtered_words = [word for word in words if word.lower() not in stop_words_en] # Convertir a minúsculas para la comparación
    return " ".join(filtered_words)

def preprocess_for_ner_inplace(df, text_column):
    """
    Aplica la limpieza y eliminación de stopwords directamente sobre la columna de texto del DataFrame.
    No se aplica lematización ni stemming para el NER.
    """
    df[text_column] = df[text_column].apply(remove_stopwords)
    df[text_column] = df[text_column].apply(clean_text_for_ner)
    return df

nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('omw-1.4', quiet=True)

stop_words_en = set(stopwords.words('english'))
df_train = preprocess_for_ner_inplace(df_train, 'text')

def extract_entities_english(text, nlp_model):
  doc = nlp_model(text)
  persons = set()        # Usamos set para evitar duplicados exactos dentro del mismo documento
  organizations = set()
  locations = set()      # Agruparemos LOC y GPE como "lugares"

  for ent in doc.ents:
      if ent.label_ == "PERSON":
          persons.add(ent.text)
      elif ent.label_ == "ORG":
          organizations.add(ent.text)
      elif ent.label_ in ["LOC", "GPE"]: # LOC -> no geopolíticos, GPE -> geopolíticos
          locations.add(ent.text)

  return list(persons), list(organizations), list(locations)

#sample_size = 1000
#df_sample = df_train.sample(n=sample_size, random_state=42)

entity_results = df_train['text'].apply(lambda text: extract_entities_english(text, nlp_en))
df_train['persons_ner'], df_train['organizations_ner'], df_train['locations_ner'] = zip(*entity_results)

print(df_train[['text', 'persons_ner', 'organizations_ner', 'locations_ner']].head())

df_train[['title','content','topic' ,'persons_ner', 'organizations_ner', 'locations_ner']].to_csv(TSV_DIR, index=False, encoding='utf-8', sep='\t')

# 4. Guardar resultado final
df_train.drop(columns=["text"], inplace=True)
df_train.to_csv(PROJECT_DIR+"/final_topic_ner.tsv", sep="\t", index=False)

